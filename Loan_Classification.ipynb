{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose: Predict the probability of a loan classification.  \n",
    "### The output is a normalized probability for each class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This model is designed to work with sparse data and imbalanced classes.  This code requires a predictor for production.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext autoreload\n",
    "#%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir loan_checkpoint\n",
    "#!mkdir loan_all_checkpoint\n",
    "#!mkdir loan_all_model\n",
    "#!mkdir loan_maps\n",
    "#!mkdir loan_val_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allow cell to perform multiple computations\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries needed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "import os as os\n",
    "import string\n",
    "from joblib import dump, load\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tensorflow & libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Embedding, Flatten, concatenate, Input\n",
    "from tensorflow.keras.models import Model, save_model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm\n",
    "from tensorflow.keras.metrics import Precision, Recall, binary_accuracy, AUC\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "dx = load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Loan Classificaton Predictor\n",
    "#### Encode, scale and shape data for tensorflow\n",
    "#### Perform train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose categorical feature vars\n",
    "cat_vars = ['','']\n",
    "\n",
    "# Choose time features vars\n",
    "time_vars = ['','']\n",
    "\n",
    "# Choose continuous feature vars\n",
    "cont_vars = ['','']\n",
    "\n",
    "# Choose target features .. can be multiple\n",
    "target = ['']\n",
    "\n",
    "# Identify data and target dataframes\n",
    "features = cat_vars + time_vars + cont_vars + target\n",
    "data = dx[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply encoders, embeddings, max/min embeddings and scalers to feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert dates to categorical variables\n",
    "def add_date_features(data,date,name):   # date is column name .. name is prefix for time feature column name e.g., \"Time_\"\n",
    "    data[name + 'Yr'] = data[date].dt.year\n",
    "    #data[name + 'Day'] = data[date].dt.dayofyear\n",
    "    #data[name + 'Week'] = data[date].dt.week\n",
    "    #data[name + 'Mon'] = data[date].dt.month \n",
    "    data[name + 'Qtr'] = data[date].dt.quarter\n",
    "    data.drop([date], axis = 1, inplace = True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute lists of embedding sizes as tuples of label encoders and scale continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply embeddings for categorical features in data\n",
    "\n",
    "def cat_map_data(data,cat_vars=cat_vars, emax=50, emin=4):\n",
    "    # compute list of number of unique categories for each categorical variable\n",
    "    cat_emb = [len(data[c].unique()) for c in cat_vars]\n",
    "    \n",
    "    # compute list inserting maximum number of embeddings for each category \n",
    "    cat_emb_max = [c if c<= emax else emax for c in cat_emb] #maximum embedded weights is emax (default=50)\n",
    "    \n",
    "    # compute list inserting minimum number of embeddings for each category\n",
    "    cat_emb_max = [c if c>= emin else emin for c in cat_emb_max] #minimum embedded weights is emin (default = 4)\n",
    "    \n",
    "    # form dictionary of the categorical variables and the list of embeddings\n",
    "    cat_vars_dict = dict(zip(cat_vars,cat_emb_max))\n",
    "    \n",
    "    # form list of tuples of categorical variables and the label encoder\n",
    "    cat_map = [(c,LabelEncoder()) for c in cat_vars]\n",
    "    \n",
    "    # return the embedding dictionary and the map of label encoders to categorical variables\n",
    "    return cat_vars_dict,cat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply embeddings for time features in data\n",
    "\n",
    "def time_map_data(data,time_vars=time_vars, tmax=12, tmin=3):\n",
    "    \n",
    "    # compute number of unique values for each time variable\n",
    "    time_emb = [len(data[t].unique()) for t in time_vars]\n",
    "    \n",
    "    # insert maximum embedded coefficients for time variables\n",
    "    time_emb_max = [t if t <= tmax else tmax for t in time_emb] #maximum embedded weights is tmax (default=12)\n",
    "    \n",
    "    # insert minimum embedded coefficients for time variables\n",
    "    time_emb_max = [t if t >= tmin else tmin for t in time_emb_max]#minimum embedded weights is tmin (default=3)\n",
    "    time_vars_dict = dict(zip(time_vars,time_emb_max))\n",
    "    \n",
    "    # compute list of tuples assigning the Label Encoder to the time variable\n",
    "    time_map = [(t,LabelEncoder()) for t in time_vars]\n",
    "    \n",
    "    # return dictionary of embedded coefficients and list of encoder tuples\n",
    "    return time_vars_dict,time_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply scaler on the continuous features in data\n",
    "\n",
    "# s can be standardscaler,robustscaler or minmaxscaler; default is minmax\n",
    "# x,y is limit on minmax; default to 1,3\n",
    "# l,u is percential rank for the robust scaler based on median; default is 10,90\n",
    "# robust scaler removes outliers before applying standard scaler on median value USE WITH CAUTION\n",
    "\n",
    "def cont_map_data(cont_vars=cont_vars, s='minmax', x=1, y=3, l=10, u=90): # s can be standardscaler,robustscaler or minmaxscaler\n",
    "    \n",
    "    # select scaler map and form list of tuples vor variable and scaler\n",
    "    if s == 'standard':\n",
    "        cont_map = [([c],StandardScaler(copy=True,with_mean=True,with_std=True)) for c in cont_vars]\n",
    "    \n",
    "    elif s == 'robust':\n",
    "        cont_map = [([c],RobustScaler(with_centering=True,with_scaling=True,quantile_range=(t,u))) for c in cont_vars]\n",
    "    \n",
    "    elif s == 'minmax':\n",
    "        cont_map = [([c],MinMaxScaler(feature_range = (x,y))) for c in cont_vars]\n",
    "    \n",
    "    # return map of scaler and continuous variables tuples\n",
    "    return cont_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform feature variables and convert to array using DataFrameMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features amd fit - transform with DataFrameMapper\n",
    "def map_cat_data(data):\n",
    "    \n",
    "    # map encoder to categorical variables; cat_vars_dict required for input layers\n",
    "    cat_vars_dict,cat_map = cat_map_data(data)\n",
    "    \n",
    "    # initialize dataframe mapper\n",
    "    cat_mapper = DataFrameMapper(cat_map)\n",
    "    \n",
    "    # fit categorical variables; cat_map_fit required for input layer\n",
    "    cat_map_fit = cat_mapper.fit(data)\n",
    "    \n",
    "    # transform data using dataframe mapper\n",
    "    cat_data = cat_map_fit.transform(data).astype(np.int64) \n",
    "    \n",
    "    # return cat_vars_dict and cat_map_fit used in embedding input layer to tensorflow\n",
    "    return  cat_data, cat_vars_dict, cat_map_fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode time features and fit - transform with DataFrameMapper    \n",
    "def map_time_data(data):    \n",
    "    \n",
    "    # map encoder to time variables; time_vars_dict used in embedding input layer to tensorflow\n",
    "    time_vars_dict,time_map = time_map_data(data)\n",
    "    \n",
    "    # initialize dataframemapper\n",
    "    time_mapper = DataFrameMapper(time_map)\n",
    "    \n",
    "    # fit time variables to data; time_map_fit req'd for input layers\n",
    "    time_map_fit = time_mapper.fit(data)\n",
    "    \n",
    "    # transform data\n",
    "    time_data = time_map_fit.transform(data).astype(np.int64)\n",
    "    \n",
    "    # return encoded time data, time_vars_dict and time_map_fit used in input layer\n",
    "    return time_data, time_vars_dict, time_map_fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale continuous features and fit with DataFrameMapper\n",
    "def map_cont_data(data):\n",
    "    \n",
    "    # map scaler to continuous data;  cont_map_data defaults to data, cont_vars and minmax scaler, x = 1,y = 3\n",
    "    cont_map = cont_map_data() \n",
    "    \n",
    "    # intialize DataFrameMapper with scalers to be applied\n",
    "    cont_mapper = DataFrameMapper(cont_map)\n",
    "    \n",
    "    # fit mapper to data; cont_map_fit is required when building input layers\n",
    "    cont_map_fit = cont_mapper.fit(data)\n",
    "    \n",
    "    # transform and return data\n",
    "    cont_data = cont_map_fit.transform(data).astype(np.float32)\n",
    "    \n",
    "    return cont_data, cont_map_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode categorical target\n",
    "##### Note that LabelEncoder will first order ascending numeric followed by ascending alphabetical.  Consequently, the encoder will assign 0 to the lowest number and then sequence 1,2,3.. to the next higher numbers and into alphabetical.  Examine class names for consistent ordering, i.e., all should begin alpha or num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode and shape categorical target tensor\n",
    "def map_cat_target(target):   \n",
    "    #target is dataframe series of shape(-1,)      \n",
    "    # map label array\n",
    "    y = np.array(target).reshape(-1,1)\n",
    "    encoder = LabelEncoder()\n",
    "    y_mapped = encoder.fit_transform(y)\n",
    "    \n",
    "    # shape y array\n",
    "    y_mapped = np.array(y_mapped).reshape(-1,1).astype(np.int64)  #tensorflow requires int64 in array\n",
    "    return y_mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute transformed and shaped input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute input layer functions\n",
    "# scale and encode data\n",
    "data_cat, cat_vars_dict, cat_map_fit = map_cat_data(data)\n",
    "data_time, time_vars_dict, time_map_fit = map_time_data(data)\n",
    "data_cont, cont_map_fit = map_cont_data(data)\n",
    "\n",
    "# concatenate scaled and encoded data\n",
    "data_scaled = np.hstack([data_cat,data_time,data_cont])\n",
    "\n",
    "# scale or encode target data\n",
    "y_scaled = map_cat_target(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train, validate and test sets.  Shape into list of arrays for tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stratified split twice on target for validation and test to balance split sizes by class counts\n",
    "# Note: splitting in tensorflow.keras.model.fit is not able to stratify by labels\n",
    "# Test sample is 5% of total sample and validation sample is 20% of remainder \n",
    "\n",
    "# first stratify and split for test data on stratify target 'y_scaled'\n",
    "data_,data_test,y_,y_test = train_test_split(data_scaled,y_scaled,test_size=.05,random_state=75,stratify=y_scaled)\n",
    "\n",
    "# resize target vector for second stratification for validation data\n",
    "temp = y_.reshape(-1,1)\n",
    "\n",
    "# second stratify and split for validation data on resized target data\n",
    "data_train,data_val,y_train,y_val = train_test_split(data_,y_,test_size=.20,random_state=75,stratify=temp)\n",
    "\n",
    "# Convert train and test input data to list of arrays for tensorflow\n",
    "X_train = np.hsplit(data_train, data_train.shape[1])\n",
    "X_val = np.hsplit(data_val, data_val.shape[1])\n",
    "X_test = np.hsplit(data_test, data_test.shape[1])\n",
    "X_all = np.hsplit(data_scaled, data_scaled.shape[1])\n",
    "\n",
    "# Convert y_train, y_val, y_test and y_all to reshaped vector arrays\n",
    "\n",
    "# win targets\n",
    "y_train = np.array(y_train).reshape(-1,1)\n",
    "y_val = np.array(y_val).reshape(-1,1)\n",
    "y_test =  np.array(y_test).reshape(-1,1)\n",
    "y_all = y_scaled.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights for tensorflow fitting method\n",
    "def category_class_weight(y):   #target is encoded category labels\n",
    "    #compute normal weight for each category\n",
    "    weights = pd.DataFrame(y.value_counts(normalize=True,ascending=False))\n",
    "    weights.reset_index(inplace=True)\n",
    "    weights.columns = ['class','frequency']\n",
    "\n",
    "    #compute frequency list\n",
    "    frequency_list = weights['frequency'].tolist()\n",
    "    frequency_list_reversed = frequency_list.reverse()\n",
    "\n",
    "    #compute class weight dictionary\n",
    "    class_weight_dict = dict(zip(weights['class'],frequency_list_reversed))\n",
    "\n",
    "    return class_weight_dict\n",
    "    \n",
    "train_weight = np.array([category_class_weight_dict[x] for x in y_train]).reshape(-1,1)\n",
    "all_weight = np.array([category_class_weight_dict[x] for x in y_all]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Loan Classification Model\n",
    "#### Plot model (if supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Embedding, Flatten, concatenate, Input\n",
    "from tensorflow.keras.models import Model, save_model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm\n",
    "from tensorflow.keras.metrics import Precision, Recall, accuracy, AUC\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy  #allows label encoded classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph categorical features input layers\n",
    "# Initialize with Xavier by calling 'glorot_normal' to minimize vanishing/exploding gradients\n",
    "# Apply batch normalization for both regularization and controlling gradients\n",
    "# Apply dropout to control overfitting\n",
    "\n",
    "# this builds input layer and subsequent layers in linear (not interconnected) configuration\n",
    "def cat_input(feat,cat_vars_dict,r=.5):\n",
    "    # compute input vector\n",
    "    name = feat[0]\n",
    "    c1 = len(feat[1].classes_)\n",
    "    c2 = cat_vars_dict[name]    \n",
    "    # create input layer\n",
    "    inp = Input(shape=(1,),dtype='int64',name=name + '_in')\n",
    "    cat = Flatten(name=name+'_flt')(Embedding(c1,c2,input_length=1)(inp))\n",
    "    cat = Dense(1000, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(cat)\n",
    "    cat = Dropout(rate=r)(cat)\n",
    "    cat = BatchNormalization()(cat)\n",
    "    cat = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(cat)\n",
    "    cat = Dropout(rate=r)(cat)\n",
    "    cat = BatchNormalization()(cat)\n",
    "    return inp,cat\n",
    "\n",
    "# Graph categorical features input\n",
    "cats = [cat_input(feat,cat_vars_dict) for feat in cat_map_fit.features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph time input layers\n",
    "# Initialize with Xavier by calling 'glorot_normal' to minimize vanishing/exploding gradients\n",
    "# Apply batch normalization for both regularization and controlling gradients\n",
    "# Apply dropout to control overfitting\n",
    "\n",
    "# this builds time input layers followed by linear layers (not interconnected) for the time variable (not interconnected)\n",
    "def time_input(feat,time_vars_dict,r=.5):\n",
    "    # compute input vector\n",
    "    name = feat[0]\n",
    "    c1 = len(feat[1].classes_)\n",
    "    c2 = time_vars_dict[name]\n",
    "    \n",
    "    # create input layer\n",
    "    inp = Input(shape=(1,),dtype='int64',name=name + '_in')\n",
    "    time = Flatten(name=name+'_flt')(Embedding(c1,c2,input_length=1)(inp))\n",
    "    time = Dense(1000, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(time)\n",
    "    time = Dropout(rate=r)(time)\n",
    "    time = BatchNormalization()(time)\n",
    "    time = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(time)\n",
    "    time = Dropout(rate=r)(time)\n",
    "    time = BatchNormalization()(time)\n",
    "    return inp,time\n",
    "\n",
    "# Graph time features input\n",
    "times = [time_input(feat,time_vars_dict) for feat in time_map_fit.features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph input layers for continuous features\n",
    "# Initialize with Xavier by calling 'glorot_normal' to minimize vanishing/exploding gradients\n",
    "# Apply batch normalization for both regularization and controlling gradients\n",
    "# Apply dropout to control overfitting\n",
    "\n",
    "# this builds input layer for continuous features and subsequent layers in linear configuration (not interconnected)\n",
    "def cont_input(feat,r=.5):\n",
    "    name = feat[0][0]\n",
    "    inp = Input((1,), name=name+'_in')\n",
    "    cont = Dense(1, name = name + '_d')(inp)\n",
    "    cont = Dense(1000, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(cont)\n",
    "    cont = Dropout(rate=r)(cont)\n",
    "    cont = BatchNormalization()(cont)\n",
    "    cont = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(cont)\n",
    "    cont = Dropout(rate=r)(cont)\n",
    "    cont = BatchNormalization()(cont)\n",
    "    return inp,cont\n",
    "\n",
    "# Graph continuous features input\n",
    "conts = [cont_input(feat) for feat in cont_map_fit.features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loan_model(cats,times,conts,r=.5):\n",
    "    \n",
    "    # Build graph for categorical features such that all categorical features are interconnected\n",
    "    c = concatenate([cat for inp,cat in cats])\n",
    "\n",
    "    # add linear layers\n",
    "    c = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(c)\n",
    "    c = Dropout(rate=r)(c)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(c)\n",
    "    c = Dropout(rate=r)(c)\n",
    "    c = BatchNormalization()(c)\n",
    "    \n",
    "    # Build graph for time features such that all time features are interconnected\n",
    "    t = concatenate([time for inp,time in times])\n",
    "    \n",
    "    # add linear layers\n",
    "    t = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(t)\n",
    "    t = Dropout(rate=r)(t)\n",
    "    t = BatchNormalization()(t)\n",
    "    t = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(t)\n",
    "    t = Dropout(rate=r)(t)\n",
    "    t = BatchNormalization()(t)\n",
    "\n",
    "    \n",
    "    # Build graph for continuous features such that all continuous features are interconnected\n",
    "    f = concatenate([cont for inp,cont in conts])\n",
    "    \n",
    "    # add linear layers\n",
    "    f = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(f)\n",
    "    f = Dropout(rate=r)(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Dense(500, activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(f)\n",
    "    f = Dropout(rate=r)(f)\n",
    "    f = BatchNormalization()(f)\n",
    "\n",
    "    \n",
    "    # Concatenate categorical, time and continuous features such that all features are interconnected\n",
    "    x = concatenate([c,t,f])\n",
    "    \n",
    "    # add linear layers\n",
    "    x = Dense(500,activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(x)\n",
    "    x = Dropout(rate=r)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(500,activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(x)\n",
    "    x = Dropout(rate=r)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(500,activation='relu',kernel_initializer='glorot_normal',bias_initializer='normal')(x)    \n",
    "    x = Dropout(rate=r)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(1, activation='sigmoid',kernel_initializer='glorot_normal',bias_initializer='normal')(x) \n",
    "    \n",
    "    # set input layer and compile\n",
    "    model = Model([inp for inp,cat in cats] + [inp for inp,time in times] + [inp for inp,cont in conts], x)\n",
    "    model.compile(optimizer='Adam',loss=SparseCrossEntropy(),metrics=['accuracy','Precision','Recall','AUC'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_model = build_loan_model(cats,times,conts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize callbacks\n",
    "# both earlystopping and modelcheckpoint save the best model to be stored\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, verbose=0,\n",
    "    mode='auto', min_delta=.005, restore_best_weights=True)\n",
    "\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.9,\n",
    "                              mode = 'min', min_delta=.005, patience=3, min_lr=0.0001)\n",
    "\n",
    "filepath = 'loan_checkpoint'\n",
    "mckp = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=True, mode='auto', save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using early stopping and reducing learning rates to reduce overfitting and 'memorization'.\n",
    "\n",
    "loan_model.fit(X_train,y_train,\n",
    "            batch_size=64,\n",
    "            epochs=20,\n",
    "            verbose=True,\n",
    "            validation_data = (X_val, y_val), \n",
    "            class_weight=train_weight,\n",
    "            callbacks=[es,rlr,mckp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display convergence of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = pd.DataFrame(data=loan_model.history.history)\n",
    "dh\n",
    "dh[['loss','val_loss']].plot()\n",
    "dh[['Precision','val_Precision']].plot()\n",
    "dh[['Recall','val_Recall']].plot()\n",
    "dh[['binary_accuracy', 'val_binary_accuracy']].plot()\n",
    "dh[['AUC','val_AUC']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display classification and confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on the validation set\n",
    "\n",
    "predicted = loan_model.predict(X_val)\n",
    "predictions = np.rint(predicted)\n",
    "print('Training Set Confusion Matrix')\n",
    "print(confusion_matrix(y_val,predictions))\n",
    "print('Training Set Classification Report')\n",
    "print(classification_report(y_val,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on the Test set\n",
    "\n",
    "predicted = loan_model.predict(X_test)\n",
    "predictions = np.rint(predicted)\n",
    "print('Test Set Confusion Matrix')\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print('Test Set Classification Report')\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The important comparison is between the training set and the test set.  Both the confusion matrix and the classification report are nearly identical for both data sets meaning that the model will generalize to new data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the predictor using all of the data set except the reserved test set for final evaluation\n",
    "\n",
    "# set the number of epochs from validation set model using early stopping\n",
    "\n",
    "e = dh.val_loss.min()\n",
    "e = dh.index[dh['val_loss']== e]\n",
    "epochs = e[0]\n",
    "\n",
    "# initiate callbacks\n",
    "\n",
    "es = EarlyStopping(monitor='loss', patience=5, verbose=0,\n",
    "    mode='auto', min_delta=.005, restore_best_weights=True)\n",
    "\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.9,\n",
    "                              mode = 'min', min_delta=.005, patience=2, min_lr=0.0001)\n",
    "\n",
    "mckp = ModelCheckpoint(filepath='loan_all_checkpoint', monitor='loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', save_freq='epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on merged train and validation set with epochs limited to the min loss epoch on validation set\n",
    "# when order_model is fit the second time, even with changed data, it loads the final weights from the first fit automatically\n",
    "# clear_session does not reset the weights in the model and the learning rate starts at the last rlr learning rate\n",
    "# \n",
    "\n",
    "loan_model.fit(X_all,y_all,\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            verbose=True,\n",
    "            validation_data = None, class_weight=all_weight,\n",
    "            callbacks=[es,rlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for prediction\n",
    "\n",
    "#filepath = 'win_all_model'\n",
    "#save_model(loan_model, filepath, overwrite=True, include_optimizer=True, save_format=None,\n",
    "#    signatures=None, options=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = pd.DataFrame(data=loan_model.history.history)\n",
    "da\n",
    "da[['binary_accuracy']].plot()\n",
    "da[['Precision']].plot()\n",
    "da[['Recall']].plot()\n",
    "da[['AUC']].plot()\n",
    "da[['loss']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display confusion and classificaton matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report and confusion matrix for fit on all data\n",
    "predicted = loan_model.predict(X_all)\n",
    "predictions = np.rint(predicted)\n",
    "print('Win All Confusion Matrix')\n",
    "print(confusion_matrix(y_all,predictions))\n",
    "print('Win All Classification Report')\n",
    "print(classification_report(y_all,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report and confusion matrix from fit on all data for test data\n",
    "predicted = loan_model.predict(X_test)\n",
    "predictions = np.rint(predicted)\n",
    "print('Loan Test Confusion Matrix')\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print('Loan Test Classification Report')\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import clear_session\n",
    "clear_session()\n",
    "del loan_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
